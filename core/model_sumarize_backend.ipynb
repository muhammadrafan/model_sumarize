{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:45:29.297062Z",
     "iopub.status.busy": "2025-05-03T08:45:29.296837Z",
     "iopub.status.idle": "2025-05-03T08:46:47.105375Z",
     "shell.execute_reply": "2025-05-03T08:46:47.104387Z",
     "shell.execute_reply.started": "2025-05-03T08:45:29.297045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting ibm-watson\n",
      "  Downloading ibm_watson-9.0.0.tar.gz (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.8/342.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.5-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting flask-cors\n",
      "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: websocket-client>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from ibm-watson) (1.8.0)\n",
      "Collecting ibm_cloud_sdk_core==3.*,>=3.3.6 (from ibm-watson)\n",
      "  Downloading ibm_cloud_sdk_core-3.23.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from ibm_cloud_sdk_core==3.*,>=3.3.6->ibm-watson) (2.3.0)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from ibm_cloud_sdk_core==3.*,>=3.3.6->ibm-watson) (2.10.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Downloading ibm_cloud_sdk_core-3.23.0-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyngrok-7.2.5-py3-none-any.whl (23 kB)\n",
      "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: ibm-watson\n",
      "  Building wheel for ibm-watson (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ibm-watson: filename=ibm_watson-9.0.0-py3-none-any.whl size=345070 sha256=7c5571e14053231f50dc38d50e77f1409032cd417fdf672ebe3ba8a8ffac6b7d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/ed/65/5abe3aa86c063331a8064910b7722d22ddf0bd75fc322f6c48\n",
      "Successfully built ibm-watson\n",
      "Installing collected packages: pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, ibm_cloud_sdk_core, nvidia-cusolver-cu12, ibm-watson, flask-cors\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flask-cors-5.0.1 ibm-watson-9.0.0 ibm_cloud_sdk_core-3.23.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyngrok-7.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy ibm-watson huggingface_hub transformers flask pyngrok flask-cors torch requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:46:47.107072Z",
     "iopub.status.busy": "2025-05-03T08:46:47.106777Z",
     "iopub.status.idle": "2025-05-03T08:46:50.450775Z",
     "shell.execute_reply": "2025-05-03T08:46:50.449988Z",
     "shell.execute_reply.started": "2025-05-03T08:46:47.107041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: Tesla T4\n",
      "CUDA version: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-03T08:56:54.904Z",
     "iopub.execute_input": "2025-05-03T08:46:50.466168Z",
     "iopub.status.busy": "2025-05-03T08:46:50.465972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 08:47:00.843901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746262021.052538      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746262021.113330      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running at: https://c411-34-91-121-186.ngrok-free.app                                        \n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      "Initializing Hugging Face client for IBM Granite...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106fd582ebe54270b42ff7b29a94de8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515622cddb2c47c8aa28440d56ee1c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d1fa7c83d14398beba3e714881e580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f38a4056b4e4991ab24a72a2685819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02006d18d1c4638a69c1042e27c1358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460d73077a2344db8e9515d0ad9fef74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de69f590a70b497085a1d63ef0fc4eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98462f2102c64622a849700f273e5247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/29.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafb176078624bf883e8ab9bfdb96ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d88a5430a53490588d36f1263b065a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d775212b0754bebb4ae0fc6f346757d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba07fe56b00b440dbbd69617079afd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d618fbf9066245f3b5d7427bdc4ad4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face client for IBM Granite initialized!\n",
      "Loading sentiment analysis model: tabularisai/multilingual-sentiment-analysis\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e7eb97ec554ea39620d3d33ea96d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65b423835c747378b728127708dea74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8842b9b1da9246e48745f9f9316c82d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4936ff440e44b2db5cf445d8f9e799e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00989fb65fcd4a7a957d1b6b1657cff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/902 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7585bd5787448582be0a40148896d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment model loaded on cuda\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n",
      "Trying to use model: ibm-granite/granite-3.3-2b-instruct\n",
      "Successfully generated summary using ibm-granite/granite-3.3-2b-instruct\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Hugging Face Hub library to access models\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Global variables to store sentiment models\n",
    "sentiment_tokenizer = None\n",
    "sentiment_model = None\n",
    "nlp_pipeline = None\n",
    "\n",
    "# Function to detect sentiment using local Hugging Face model\n",
    "def analyze_sentiment(hf_client, text, sentiment_model_id=\"tabularisai/multilingual-sentiment-analysis\"):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment from text using local sentiment model with AutoModelForSequenceClassification\n",
    "    \"\"\"\n",
    "    global sentiment_tokenizer, sentiment_model\n",
    "    \n",
    "    if not text or not isinstance(text, str) or len(text.strip()) < 5:\n",
    "        return {'score': 0, 'label': 'neutral'}\n",
    "    \n",
    "    try:\n",
    "        # Initialize sentiment models if not already done\n",
    "        if sentiment_tokenizer is None or sentiment_model is None:\n",
    "            print(f\"Loading sentiment analysis model: {sentiment_model_id}\")\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_id)\n",
    "            sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_id).to(device)\n",
    "            print(f\"Sentiment model loaded on {device}\")\n",
    "        \n",
    "        # Get the device where the model is loaded\n",
    "        device = next(sentiment_model.parameters()).device\n",
    "        \n",
    "        # Tokenize and get prediction\n",
    "        encoded_input = sentiment_tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "        output = sentiment_model(**encoded_input)\n",
    "        scores = softmax(output.logits, dim=1).detach().cpu().numpy()[0]\n",
    "        \n",
    "        # Get prediction\n",
    "        predicted_class = torch.argmax(output.logits, dim=1).item()\n",
    "        id2label = sentiment_model.config.id2label\n",
    "        label = id2label[predicted_class].lower()\n",
    "        score = float(scores[predicted_class])\n",
    "        \n",
    "        # Adjust score format to match the original function\n",
    "        sentiment = {'label': label}\n",
    "        if label == 'positive':\n",
    "            sentiment['score'] = score\n",
    "        elif label == 'negative':\n",
    "            sentiment['score'] = -score\n",
    "        else:\n",
    "            sentiment['score'] = 0\n",
    "            \n",
    "        return sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment with local model: {e}\")\n",
    "        # Fallback to API call if local model fails\n",
    "        try:\n",
    "            print(\"Falling back to API client for sentiment analysis\")\n",
    "            result = hf_client.text_classification(text, model=sentiment_model_id)\n",
    "            \n",
    "            # Adjust output format\n",
    "            sentiment = {'label': result[0]['label'].lower()}\n",
    "            if sentiment['label'] == 'positive':\n",
    "                sentiment['score'] = result[0]['score']\n",
    "            elif sentiment['label'] == 'negative':\n",
    "                sentiment['score'] = -result[0]['score']\n",
    "            else:\n",
    "                sentiment['score'] = 0\n",
    "                \n",
    "            return sentiment\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fallback sentiment analysis: {e}\")\n",
    "            return {'score': 0, 'label': 'neutral'}\n",
    "\n",
    "# Function to load Hugging Face client\n",
    "def load_hf_client(hf_token):\n",
    "    \"\"\"\n",
    "    Initialize Hugging Face client and pipeline for IBM Granite\n",
    "    \"\"\"\n",
    "    global nlp_pipeline\n",
    "    \n",
    "    print(\"Initializing Hugging Face client for IBM Granite...\")\n",
    "    \n",
    "    # Initialize inference client for API calls\n",
    "    client = InferenceClient(token=hf_token)\n",
    "    \n",
    "    # Model ID for IBM Granite\n",
    "    model_id = \"ibm-granite/granite-3.3-2b-instruct\"\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize tokenizer and model using AutoTokenizer and AutoModelForCausalLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    \n",
    "    # Initialize pipeline for text generation\n",
    "    nlp_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    print(\"Hugging Face client for IBM Granite initialized!\")\n",
    "    return client, model_id, tokenizer, model\n",
    "\n",
    "# Function to prepare performance data for prompt\n",
    "def prepare_performance_data(employee_data, comparative_data=None):\n",
    "    \"\"\"\n",
    "    Format the employee performance data for the prompt\n",
    "    \"\"\"\n",
    "    # Format performance data \n",
    "    performance_text = f\"\"\"\n",
    "    Employee Name: {employee_data['Employee Name']}\n",
    "    Employee ID: {employee_data['Employee ID']}\n",
    "    \n",
    "    WEEKLY KPIs:\n",
    "    - Productivity: {employee_data['Productivity: Number of tasks completed']} tasks completed, {employee_data['Productivity: Time to complete tasks (hours/task)']:.2f} hours/task\n",
    "    - Work Quality: Error rate {employee_data['Quality of Work: Error rate (%)']:.2f}%, Customer satisfaction {employee_data['Quality of Work: Customer satisfaction rate (%)']:.2f}%\n",
    "    - Attendance & Punctuality: Attendance {employee_data['Presence and Punctuality: Attendance rate (%)']:.2f}%, Punctuality {employee_data['Presence and Punctuality: Punctuality rate (%)']:.2f}%\n",
    "    - Goals & Objectives: Individual achievement {employee_data['Goals and Objectives: Individual goal achievement (%)']:.2f}%, Team achievement {employee_data['Goals and Objectives: Team goal achievement (%)']:.2f}%, Contribution {employee_data['Goals and Objectives: Contribution to company vision (1-5)']:.2f}/5\n",
    "    - Collaboration & Teamwork: Communication {employee_data['Collaboration and Teamwork: Communication skills (1-5)']:.2f}/5, Teamwork {employee_data['Collaboration and Teamwork: Ability to work in a team (1-5)']:.2f}/5\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add comparative data if available\n",
    "    if comparative_data is not None:\n",
    "        performance_text += f\"\"\"\n",
    "        COMPARISON WITH PREVIOUS WEEK:\n",
    "        - Change in number of tasks: {employee_data['Productivity: Number of tasks completed'] - comparative_data['Productivity: Number of tasks completed']}\n",
    "        - Change in time per task: {employee_data['Productivity: Time to complete tasks (hours/task)'] - comparative_data['Productivity: Time to complete tasks (hours/task)']:.2f} hours\n",
    "        - Change in error rate: {employee_data['Quality of Work: Error rate (%)'] - comparative_data['Quality of Work: Error rate (%)']:.2f}%\n",
    "        - Change in customer satisfaction: {employee_data['Quality of Work: Customer satisfaction rate (%)'] - comparative_data['Quality of Work: Customer satisfaction rate (%)']:.2f}%\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add survey data if available\n",
    "    if 'survey_data' in employee_data:\n",
    "        survey = employee_data['survey_data']\n",
    "        performance_text += f\"\"\"\n",
    "        MONTHLY SURVEY:\n",
    "        - Self-Performance: {survey.get('Self-Performance', 'No data')}\n",
    "        - Goals Achieved: {survey.get('Goals Achieved', 'No data')}\n",
    "        - Personal Challenges: {survey.get('Personal Challenges', 'No data')}\n",
    "        - Stress/Anxiety: {survey.get('Stress or Anxiety', 'No data')}\n",
    "        - Relationship with Colleagues: {survey.get('Relationship with Colleagues', 'No data')}\n",
    "        - Communication Issues: {survey.get('Communication Issues', 'No data')}\n",
    "        - Team Conflicts: {survey.get('Team Conflicts', 'No data')}\n",
    "        - Team Collaboration: {survey.get('Team Collaboration', 'No data')}\n",
    "        \"\"\"\n",
    "    \n",
    "    return performance_text\n",
    "\n",
    "# Function to create prompt for model\n",
    "def create_summary_prompt(performance_text):\n",
    "    \"\"\"\n",
    "    Create the prompt for the model using performance data\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an HR assistant expert in analyzing employee performance.\n",
    "    \n",
    "    Task:\n",
    "    Analyze the following employee performance data and provide a summary that assesses:\n",
    "    1. Whether performance is good or poor (compare with threshold: min. 15 tasks, max. 3 hours/task, max. 5% error, min. 80% satisfaction)\n",
    "    2. How it compares to the previous week (up/down)\n",
    "    3. Which areas need improvement\n",
    "    4. Whether this employee needs a personal psychologist or conflict resolution (based on survey)\n",
    "    \n",
    "    EMPLOYEE DATA:\n",
    "    {performance_text}\n",
    "    \n",
    "    Output format:\n",
    "    Performance Summary: [good/poor and explanation]\n",
    "    Comparison: [summary comparison with previous week]\n",
    "    Improvement Areas: [1-3 main areas that need improvement]\n",
    "    Recommendation: [psychologist/conflict resolution/not needed] and reason\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Function to extract summary from model response\n",
    "def extract_summary(response):\n",
    "    \"\"\"\n",
    "    Extract and format the summary from the model response\n",
    "    \"\"\"\n",
    "    # Remove any prompt text that might have been included in the response\n",
    "    if \"You are an HR assistant\" in response:\n",
    "        response = response.split(\"You are an HR assistant\", 1)[0]\n",
    "    \n",
    "    if \"EMPLOYEE DATA:\" in response:\n",
    "        response = response.split(\"EMPLOYEE DATA:\", 1)[0]\n",
    "    \n",
    "    if \"Output format:\" in response:\n",
    "        response = response.split(\"Output format:\", 1)[1]\n",
    "    \n",
    "    # Clean up and format the summary\n",
    "    summary = response.strip()\n",
    "    \n",
    "    # Ensure the response includes the required sections\n",
    "    sections = [\"Performance Summary:\", \"Comparison:\", \"Improvement Areas:\", \"Recommendation:\"]\n",
    "    \n",
    "    formatted_summary = {}\n",
    "    for section in sections:\n",
    "        if section in summary:\n",
    "            section_index = summary.find(section)\n",
    "            next_section_index = float('inf')\n",
    "            \n",
    "            for next_section in sections:\n",
    "                if next_section != section and next_section in summary and summary.find(next_section) > section_index:\n",
    "                    next_section_index = min(next_section_index, summary.find(next_section))\n",
    "            \n",
    "            if next_section_index == float('inf'):\n",
    "                section_content = summary[section_index + len(section):].strip()\n",
    "            else:\n",
    "                section_content = summary[section_index + len(section):next_section_index].strip()\n",
    "            \n",
    "            formatted_summary[section.replace(\":\", \"\")] = section_content\n",
    "        else:\n",
    "            formatted_summary[section.replace(\":\", \"\")] = \"No information available\"\n",
    "    \n",
    "    return formatted_summary\n",
    "\n",
    "# Function to summarize employee performance using pipeline approach\n",
    "def summarize_employee_performance(hf_client, model_id, employee_data, comparative_data=None, tokenizer=None, model=None):\n",
    "    \"\"\"\n",
    "    Summarize employee performance report using pipeline approach with fallback strategy\n",
    "    \"\"\"\n",
    "    global nlp_pipeline\n",
    "    \n",
    "    # PIPELINE APPROACH:\n",
    "    # 1. Prepare the performance data\n",
    "    performance_text = prepare_performance_data(employee_data, comparative_data)\n",
    "    \n",
    "    # 2. Create the prompt for the model\n",
    "    prompt = create_summary_prompt(performance_text)\n",
    "    \n",
    "    # 3. Generate summary using model with fallback options\n",
    "    # List of models to try, from most preferred to fallback\n",
    "    models_to_try = [\n",
    "        model_id,  # IBM Granite (original)\n",
    "        \"google/flan-t5-large\",  # Fallback option 1\n",
    "        \"google/flan-t5-base\",   # Fallback option 2\n",
    "        \"facebook/bart-large-cnn\"  # Fallback option 3\n",
    "    ]\n",
    "    \n",
    "    # Parameter options for various models\n",
    "    model_params = {\n",
    "        model_id: {\n",
    "            \"max_new_tokens\": 250,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": True\n",
    "        },\n",
    "        \"google/flan-t5-large\": {\n",
    "            \"max_new_tokens\": 250,\n",
    "            \"temperature\": 0.3\n",
    "        },\n",
    "        \"google/flan-t5-base\": {\n",
    "            \"max_new_tokens\": 250,\n",
    "            \"temperature\": 0.3\n",
    "        },\n",
    "        \"facebook/bart-large-cnn\": {\n",
    "            \"max_new_tokens\": 250,\n",
    "            \"temperature\": 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Try models one by one until successful\n",
    "    last_error = None\n",
    "    for model_choice in models_to_try:\n",
    "        try:\n",
    "            print(f\"Trying to use model: {model_choice}\")\n",
    "            params = model_params.get(model_choice, {\"max_new_tokens\": 250, \"temperature\": 0.3})\n",
    "            \n",
    "            # If using the primary model and we have pipeline initialized\n",
    "            if model_choice == model_id and nlp_pipeline is not None:\n",
    "                # Use pipeline approach\n",
    "                result = nlp_pipeline(\n",
    "                    prompt,\n",
    "                    max_new_tokens=params[\"max_new_tokens\"],\n",
    "                    temperature=params[\"temperature\"],\n",
    "                    top_p=params.get(\"top_p\", 0.95),\n",
    "                    do_sample=params.get(\"do_sample\", True)\n",
    "                )\n",
    "                response = result[0]['generated_text']\n",
    "                \n",
    "                # Extract just the generated part (remove the prompt)\n",
    "                if prompt in response:\n",
    "                    response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            # If using the primary model and we have tokenizer/model loaded locally\n",
    "            elif model_choice == model_id and tokenizer is not None and model is not None:\n",
    "                # Use local AutoTokenizer and AutoModelForCausalLM with GPU support\n",
    "                device = next(model.parameters()).device  # Get the device the model is on\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=params[\"max_new_tokens\"],\n",
    "                    temperature=params[\"temperature\"],\n",
    "                    top_p=params.get(\"top_p\", 0.95),\n",
    "                    do_sample=params.get(\"do_sample\", True)\n",
    "                )\n",
    "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Extract just the generated part (remove the prompt)\n",
    "                if prompt in response:\n",
    "                    response = response.replace(prompt, \"\").strip()\n",
    "            else:\n",
    "                # Use API client as fallback\n",
    "                response = hf_client.text_generation(\n",
    "                    prompt=prompt,\n",
    "                    model=model_choice,\n",
    "                    **params\n",
    "                )\n",
    "            \n",
    "            print(f\"Successfully generated summary using {model_choice}\")\n",
    "            \n",
    "            # 4. Extract and format the summary\n",
    "            formatted_summary = extract_summary(response)\n",
    "            \n",
    "            # Return the formatted summary\n",
    "            return formatted_summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"Error with model {model_choice}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # If all models fail, create a simple rule-based summary\n",
    "    print(\"All models failed. Generating rule-based summary.\")\n",
    "    \n",
    "    # Analyze metrics based on thresholds\n",
    "    performance_rating = \"good\" if len(employee_data.get('bad_metrics', [])) == 0 else \"poor\"\n",
    "    \n",
    "    # Compare with previous week if available\n",
    "    comparison = \"No comparison data available.\" \n",
    "    if comparative_data is not None:\n",
    "        task_diff = employee_data['Productivity: Number of tasks completed'] - comparative_data['Productivity: Number of tasks completed']\n",
    "        error_diff = employee_data['Quality of Work: Error rate (%)'] - comparative_data['Quality of Work: Error rate (%)']\n",
    "        \n",
    "        if task_diff > 0 and error_diff < 0:\n",
    "            comparison = f\"Performance improved (tasks +{task_diff}, error {error_diff:.2f}%).\"\n",
    "        elif task_diff < 0 or error_diff > 0:\n",
    "            comparison = f\"Performance declined (tasks {task_diff}, error {error_diff:.2f}%).\"\n",
    "        else:\n",
    "            comparison = \"Performance relatively stable compared to previous week.\"\n",
    "    \n",
    "    # Improvement areas\n",
    "    areas = employee_data.get('bad_metrics', [])\n",
    "    improvement = \"No areas requiring urgent improvement.\" if not areas else f\"Needs improvement in: {', '.join(areas)}.\"\n",
    "    \n",
    "    # Recommendation\n",
    "    recommendation = \"Not needed\"\n",
    "    if employee_data.get('need_psychologist', False):\n",
    "        recommendation = \"Psychologist - signs of stress/anxiety detected\"\n",
    "    elif employee_data.get('need_conflict_resolution', False):\n",
    "        recommendation = \"Conflict resolution - signs of team conflict detected\"\n",
    "    \n",
    "    # Create manual summary in formatted structure\n",
    "    rule_based_summary = {\n",
    "        \"Performance Summary\": performance_rating,\n",
    "        \"Comparison\": comparison,\n",
    "        \"Improvement Areas\": improvement,\n",
    "        \"Recommendation\": recommendation\n",
    "    }\n",
    "    \n",
    "    return rule_based_summary\n",
    "\n",
    "# Function to process data and generate summary\n",
    "def process_employee_data(kpi_week1_df, kpi_week2_df, survey_df, hf_client, model_id, tokenizer=None, model=None, employee_id=None):\n",
    "    \"\"\"\n",
    "    Process employee data and generate performance summaries\n",
    "    \"\"\"\n",
    "    # Thresholds to determine if performance is good or poor\n",
    "    thresholds = {\n",
    "        'tasks_completed': 15,  # Minimum tasks to be completed\n",
    "        'time_per_task': 3,     # Maximum time per task (hours)\n",
    "        'error_rate': 5,        # Maximum error rate (%)\n",
    "        'customer_satisfaction': 80,  # Minimum customer satisfaction (%)\n",
    "    }\n",
    "    \n",
    "    all_summaries = {}\n",
    "    \n",
    "    # If employee_id is provided, only process that employee\n",
    "    if employee_id:\n",
    "        employee_ids = [employee_id]\n",
    "    else:\n",
    "        employee_ids = kpi_week2_df['Employee ID'].unique()\n",
    "    \n",
    "    # Iterate through each employee\n",
    "    for emp_id in employee_ids:\n",
    "        try:\n",
    "            # Get employee data from all sources\n",
    "            if emp_id not in kpi_week2_df['Employee ID'].values:\n",
    "                continue\n",
    "                \n",
    "            emp_week2 = kpi_week2_df[kpi_week2_df['Employee ID'] == emp_id].iloc[0].to_dict()\n",
    "            emp_week1 = kpi_week1_df[kpi_week1_df['Employee ID'] == emp_id].iloc[0].to_dict() if emp_id in kpi_week1_df['Employee ID'].values else None\n",
    "            emp_survey = survey_df[survey_df['Employee ID'] == emp_id].iloc[0].to_dict() if emp_id in survey_df['Employee ID'].values else None\n",
    "\n",
    "            # Merge survey data to week 2 KPI data if available\n",
    "            if emp_survey is not None:\n",
    "                emp_week2['survey_data'] = emp_survey\n",
    "            \n",
    "            # Evaluate employee performance\n",
    "            bad_metrics = []\n",
    "            if emp_week2['Productivity: Number of tasks completed'] < thresholds['tasks_completed']:\n",
    "                bad_metrics.append('number of tasks')\n",
    "            if emp_week2['Productivity: Time to complete tasks (hours/task)'] > thresholds['time_per_task']:\n",
    "                bad_metrics.append('time per task')\n",
    "            if emp_week2['Quality of Work: Error rate (%)'] > thresholds['error_rate']:\n",
    "                bad_metrics.append('error rate')\n",
    "            if emp_week2['Quality of Work: Customer satisfaction rate (%)'] < thresholds['customer_satisfaction']:\n",
    "                bad_metrics.append('customer satisfaction')\n",
    "            \n",
    "            # Detect issues from survey\n",
    "            need_psychologist = False\n",
    "            need_conflict_resolution = False\n",
    "            \n",
    "            if emp_survey is not None:\n",
    "                # Sentiment analysis for personal and team issues\n",
    "                stress_text = str(emp_survey.get('Stress or Anxiety', ''))\n",
    "                conflict_text = str(emp_survey.get('Team Conflicts', ''))\n",
    "                \n",
    "                if len(stress_text) > 5:\n",
    "                    stress_analysis = analyze_sentiment(hf_client, stress_text)\n",
    "                    need_psychologist = stress_analysis['label'] == 'negative' and stress_analysis['score'] < -0.3\n",
    "                \n",
    "                if len(conflict_text) > 5:\n",
    "                    conflict_analysis = analyze_sentiment(hf_client, conflict_text)\n",
    "                    need_conflict_resolution = conflict_analysis['label'] == 'negative' and conflict_analysis['score'] < -0.3\n",
    "            \n",
    "            # Add evaluation results to employee data\n",
    "            emp_week2['bad_metrics'] = bad_metrics\n",
    "            emp_week2['need_psychologist'] = need_psychologist\n",
    "            emp_week2['need_conflict_resolution'] = need_conflict_resolution\n",
    "            \n",
    "            # Generate summary with IBM Granite via Hugging Face\n",
    "            summary_data = summarize_employee_performance(hf_client, model_id, emp_week2, emp_week1, tokenizer, model)\n",
    "            \n",
    "            # Create a combined summary string from the structured data\n",
    "            combined_summary = \"\\n\".join([f\"{key}: {value}\" for key, value in summary_data.items()])\n",
    "            \n",
    "            # Store summary\n",
    "            all_summaries[emp_id] = {\n",
    "                'employee_name': emp_week2['Employee Name'],\n",
    "                'employee_id': emp_id,\n",
    "                'summary': combined_summary,\n",
    "                'summary_data': summary_data,  # Add the structured summary data as a new column\n",
    "                'need_psychologist': need_psychologist,\n",
    "                'need_conflict_resolution': need_conflict_resolution,\n",
    "                'bad_metrics': bad_metrics\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing employee {emp_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_summaries\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global variables to store data and model\n",
    "kpi_week1_df = None\n",
    "kpi_week2_df = None\n",
    "survey_df = None\n",
    "hf_client = None\n",
    "model_id = None\n",
    "tokenizer = None\n",
    "model = None\n",
    "sentiment_tokenizer = None\n",
    "sentiment_model = None\n",
    "\n",
    "# Route for file upload\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_files():\n",
    "    global kpi_week1_df, kpi_week2_df, survey_df\n",
    "    \n",
    "    try:\n",
    "        # Check if files are received\n",
    "        if 'kpi_week1' not in request.files or 'kpi_week2' not in request.files or 'survey' not in request.files:\n",
    "            return jsonify({'error': 'Missing required files'}), 400\n",
    "            \n",
    "        # Save files to temporary directory\n",
    "        kpi_week1_file = request.files['kpi_week1']\n",
    "        kpi_week2_file = request.files['kpi_week2']\n",
    "        survey_file = request.files['survey']\n",
    "        \n",
    "        # Read CSV files\n",
    "        kpi_week1_df = pd.read_csv(kpi_week1_file)\n",
    "        kpi_week2_df = pd.read_csv(kpi_week2_file)\n",
    "        survey_df = pd.read_csv(survey_file)\n",
    "        \n",
    "        return jsonify({\n",
    "            'message': 'Files uploaded successfully',\n",
    "            'employee_count': len(kpi_week2_df['Employee ID'].unique()),\n",
    "            'kpi_week1_shape': kpi_week1_df.shape,\n",
    "            'kpi_week2_shape': kpi_week2_df.shape,\n",
    "            'survey_shape': survey_df.shape\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# Route for model initialization\n",
    "@app.route('/init_model', methods=['POST'])\n",
    "def init_model():\n",
    "    global hf_client, model_id, tokenizer, model, sentiment_tokenizer, sentiment_model, nlp_pipeline\n",
    "    \n",
    "    try:\n",
    "        data = request.json\n",
    "        hf_token = 'your_token_here'  # Replace with your actual Hugging Face token\n",
    "        \n",
    "        # Load Hugging Face client and model\n",
    "        hf_client, model_id, tokenizer, model = load_hf_client(hf_token)\n",
    "        \n",
    "        # Initialize sentiment model\n",
    "        sentiment_model_id = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Loading sentiment analysis model: {sentiment_model_id}\")\n",
    "        sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_id)\n",
    "        sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_id).to(device)\n",
    "        print(f\"Sentiment model loaded on {device}\")\n",
    "        \n",
    "        return jsonify({'message': 'Hugging Face client and models initialized successfully'})\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# Route for processing data and generating summaries\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process_data():\n",
    "    global kpi_week1_df, kpi_week2_df, survey_df, hf_client, model_id, tokenizer, model\n",
    "    \n",
    "    try:\n",
    "        # Check if data and model are initialized\n",
    "        if kpi_week1_df is None or kpi_week2_df is None or survey_df is None:\n",
    "            return jsonify({'error': 'Data not uploaded yet'}), 400\n",
    "            \n",
    "        if hf_client is None:\n",
    "            return jsonify({'error': 'Hugging Face client not initialized yet'}), 400\n",
    "        \n",
    "        # Get employee_id from request if present\n",
    "        data = request.json\n",
    "        employee_id = data.get('employee_id', None)\n",
    "        \n",
    "        # Process data and generate summaries\n",
    "        summaries = process_employee_data(kpi_week1_df, kpi_week2_df, survey_df, hf_client, model_id, tokenizer, model, employee_id)\n",
    "        \n",
    "        return jsonify({\n",
    "            'message': 'Processing completed successfully',\n",
    "            'summaries': summaries\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# Route for checking status\n",
    "@app.route('/status', methods=['GET'])\n",
    "def check_status():\n",
    "    status = {\n",
    "        'data_loaded': {\n",
    "            'kpi_week1': kpi_week1_df is not None,\n",
    "            'kpi_week2': kpi_week2_df is not None,\n",
    "            'survey': survey_df is not None\n",
    "        },\n",
    "        'models_loaded': {\n",
    "            'hf_client': hf_client is not None,\n",
    "            'tokenizer': tokenizer is not None,\n",
    "            'model': model is not None,\n",
    "            'nlp_pipeline': nlp_pipeline is not None,\n",
    "            'sentiment_tokenizer': sentiment_tokenizer is not None,\n",
    "            'sentiment_model': sentiment_model is not None\n",
    "        },\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_info': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'\n",
    "    }\n",
    "    \n",
    "    return jsonify(status)\n",
    "\n",
    "# Initialize ngrok and run app\n",
    "def run_app():\n",
    "    # Configure ngrok\n",
    "    ngrok_auth_token = 'your_ngrok_auth_token_here'  # Replace with your actual ngrok auth token\n",
    "    if not ngrok_auth_token:\n",
    "        raise ValueError(\"Please set your ngrok auth token.\")\n",
    "    ngrok.set_auth_token(ngrok_auth_token)\n",
    "    \n",
    "    # Open ngrok tunnel\n",
    "    public_url = ngrok.connect(5000).public_url\n",
    "    print(f\"Server running at: {public_url}\")\n",
    "    \n",
    "    # Run app\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    run_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
